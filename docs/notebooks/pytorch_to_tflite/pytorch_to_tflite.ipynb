{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c42ac65",
   "metadata": {},
   "source": [
    "# Converting PyTorch to TensorFlow Lite for xCORE Using ONNX\n",
    "\n",
    "ONNX is an open format built to represent machine learning models. We can convert from PyTorch to ONNX, then from ONNX to TensorFlow, then from TensorFlow to TensorFlow Lite, and finally, run it through xformer to optimise it for xCORE.\n",
    "Ensure that you have installed Python 3.8 and have the installed requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90a93c",
   "metadata": {},
   "source": [
    "## Import PyTorch Model\n",
    "\n",
    "For this example, we use mobilenet_v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f69af6e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/salmankhan/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "pytorch_model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "\n",
    "# Switch the model to eval mode\n",
    "pytorch_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2033740",
   "metadata": {},
   "source": [
    "### Run inference on model (to test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10252ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an image to test against\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d609035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-28 18:20:48--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10472 (10K) [text/plain]\n",
      "Saving to: ‘imagenet_classes.txt.18’\n",
      "\n",
      "imagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2023-03-28 18:20:49 (14.0 MB/s) - ‘imagenet_classes.txt.18’ saved [10472/10472]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download Image Labels\n",
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ccda852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2630c0",
   "metadata": {},
   "source": [
    "### Perform an infrence on the pytorch model directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f2b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will test and train with these params\n",
    "batch_size = 1\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb63c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Open testing image\n",
    "input_image = Image.open(filename)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(height),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Note Pytorch is BCHW\n",
    "input_tensor = preprocess(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8424fc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samoyed : 0.8303043246269226\n",
      "Pomeranian : 0.06988773494958878\n",
      "keeshond : 0.012964080087840557\n",
      "collie : 0.010797776281833649\n",
      "Great Pyrenees : 0.009886783547699451\n"
     ]
    }
   ],
   "source": [
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = pytorch_model(input_batch)\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "# Show top categories per image\n",
    "vals, idxs = torch.topk(probabilities, 5)\n",
    "pytorch_results = [(categories[idx], prob) for (idx, prob) in zip(idxs.tolist(), vals.tolist())]\n",
    "for cat, prob in  pytorch_results:\n",
    "    print(cat, ':', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fde0a",
   "metadata": {},
   "source": [
    "## Convert to ONNX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a13b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only for shape info for tracnig the model during conversion\n",
    "sample_input = torch.rand((batch_size, channels, height, width))\n",
    "\n",
    "onnx_model_path = \"mobilenet_v2.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    pytorch_model,\n",
    "    sample_input,\n",
    "    onnx_model_path,\n",
    "    input_names=['image'],\n",
    "    output_names=['probabilities']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6475e32",
   "metadata": {},
   "source": [
    "### Check the exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bdc8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d2393",
   "metadata": {},
   "source": [
    "### Check ONNX Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "665cc98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samoyed : 0.830305\n",
      "Pomeranian : 0.06988746\n",
      "keeshond : 0.012964004\n",
      "collie : 0.010797733\n",
      "Great Pyrenees : 0.009886744\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime \n",
    "import numpy as np\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def softmax(xs):\n",
    "    return np.exp(xs)/sum(np.exp(xs))\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "input_batch_np = to_numpy(input_batch)\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: input_batch_np}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# The input is still BCHW\n",
    "data = zip(range(len(ort_outs[0][0])), softmax(ort_outs[0][0]))\n",
    "\n",
    "onnx_results = [(categories[idx], prob) for (idx, prob) in sorted(data, key=lambda x: x[1], reverse=True)[:5]]\n",
    "for cat, prob in  onnx_results:\n",
    "    print(cat, ':', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db0f492",
   "metadata": {},
   "source": [
    "## Using onnx2tf\n",
    "\n",
    "Using package: https://github.com/PINTO0309/onnx2tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5ad56",
   "metadata": {},
   "source": [
    "### Convert ONNX to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2722c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n"
     ]
    }
   ],
   "source": [
    "import onnx2tf\n",
    "\n",
    "keras_model_path = 'mobilenet_v2.tf'\n",
    "\n",
    "keras_model = onnx2tf.convert(\n",
    "    input_onnx_file_path=onnx_model_path,\n",
    "    output_folder_path=keras_model_path,\n",
    "    non_verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549092a",
   "metadata": {},
   "source": [
    "### Check the conversion to keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72cdf53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samoyed : 0.83030456\n",
      "Pomeranian : 0.069887355\n",
      "keeshond : 0.01296406\n",
      "collie : 0.01079778\n",
      "Great Pyrenees : 0.009886805\n"
     ]
    }
   ],
   "source": [
    "#transpose the input_batch into BHWC order for tensorflow\n",
    "tf_input_data = np.transpose( input_batch.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "keras_output_data = keras_model(tf_input_data)\n",
    "\n",
    "probs = softmax(keras_output_data[0])\n",
    "data = zip(range(len(probs)), probs)\n",
    "keras_results = [(categories[idx], prob) for (idx, prob) in sorted(data, key=lambda x: x[1], reverse=True)[:5]]\n",
    "for cat, prob in  keras_results:\n",
    "    print(cat, ':', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f3aa5",
   "metadata": {},
   "source": [
    "## Convert Keras to TFLite (float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b392bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "converter.inference_input_type = tf.float32 \n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_float_model_path = 'mobilenet_v2_float.tflite'\n",
    "with open(tflite_float_model_path, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba66dc2b",
   "metadata": {},
   "source": [
    "### Check it Worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a1fce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samoyed : 0.84704113\n",
      "Pomeranian : 0.060772426\n",
      "keeshond : 0.012674405\n",
      "collie : 0.00844481\n",
      "Great Pyrenees : 0.008021476\n"
     ]
    }
   ],
   "source": [
    "tfl_interpreter = tf.lite.Interpreter(model_path=tflite_float_model_path)\n",
    "tfl_interpreter.allocate_tensors()\n",
    "\n",
    "tfl_input_details = tfl_interpreter.get_input_details()\n",
    "tfl_output_details = tfl_interpreter.get_output_details()\n",
    "\n",
    "# Convert PyTorch Input Tensor into Numpy Matrix and Reshape for TensorFlow\n",
    "# (Pytorch model expects C x H x W but TF expects H x W x C)\n",
    "tfl_interpreter.set_tensor(tfl_input_details[0]['index'], tf_input_data)\n",
    "tfl_interpreter.invoke()\n",
    "\n",
    "tfl_output_data = tfl_interpreter.get_tensor(tfl_output_details[0]['index'])\n",
    "\n",
    "probs = softmax(tfl_output_data[0])\n",
    "data = zip(range(len(probs)), probs)\n",
    "tfl_float32_results = [(categories[idx], prob) for (idx, prob) in sorted(data, key=lambda x: x[1], reverse=True)[:5]]\n",
    "for cat, prob in  tfl_float32_results:\n",
    "    print(cat, ':', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497e02a",
   "metadata": {},
   "source": [
    "## Convert Keras to TFLite (int8)\n",
    "We will still feed the data into the model in float32 format for convinence but the internals of the model will be int8. This will require representitive data but as we interface in float32 we can use the pytorch preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1db68",
   "metadata": {},
   "source": [
    "## Representative Dataset\n",
    "\n",
    "To convert a model into to a TFLite flatbuffer, a representative dataset is required to help in quantisation. Refer to [Converting a keras model into an xcore optimised tflite model](https://colab.research.google.com/github/xmos/ai_tools/blob/develop/docs/notebooks/keras_to_xcore.ipynb) for more details on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a5c3e",
   "metadata": {},
   "source": [
    "#### Download & Extract Images from Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15753408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ./imagenet-dataset: File exists\n",
      "--2023-03-28 18:21:46--  https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.171.5, 52.217.111.62, 52.217.225.136, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.171.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 341663724 (326M) [application/x-tar]\n",
      "Saving to: ‘./imagenet-dataset/imagenette2-320.tgz’\n",
      "\n",
      "./imagenet-dataset/ 100%[===================>] 325.83M  7.95MB/s    in 47s     \n",
      "\n",
      "2023-03-28 18:22:34 (6.90 MB/s) - ‘./imagenet-dataset/imagenette2-320.tgz’ saved [341663724/341663724]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will use the imagenette dataset(326MB)\n",
    "!mkdir ./imagenet-dataset\n",
    "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz -O ./imagenet-dataset/imagenette2-320.tgz\n",
    "!tar -xf ./imagenet-dataset/imagenette2-320.tgz -C ./imagenet-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32503a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "all_files = glob.glob(os.path.join(\"./imagenet-dataset/\", '**/*.JPEG'), recursive=True)\n",
    "\n",
    "# Randomly select a subset of images, let's just use 1k for speed\n",
    "sampled_files = random.sample(all_files, 1000)\n",
    "\n",
    "def representative_dataset():\n",
    "\n",
    "    # Iterate over the sampled images and preprocess them\n",
    "    for image_path in sampled_files:\n",
    "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "        pytorch_tensor = preprocess(pil_image).unsqueeze(0)\n",
    "        img_array_np = pytorch_tensor.numpy()\n",
    "        \n",
    "        #swap the axes to BHWC(tf) from BCHW(pytorch)\n",
    "        img_array_np = np.transpose( img_array_np, [0, 2, 3, 1])\n",
    "        yield [img_array_np]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61799382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# Now do the conversion to int8\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32 \n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_int8_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_int8_model_path = 'mobilenet_v2.tflite'\n",
    "with open(tflite_int8_model_path, 'wb') as f:\n",
    "  f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38fd12d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samoyed : 0.8527114\n",
      "Pomeranian : 0.075921215\n",
      "keeshond : 0.018518537\n",
      "Arctic fox : 0.0067596436\n",
      "West Highland white terrier : 0.0055256966\n"
     ]
    }
   ],
   "source": [
    "tfl_interpreter = tf.lite.Interpreter(model_path=tflite_int8_model_path)\n",
    "tfl_interpreter.allocate_tensors()\n",
    "\n",
    "tfl_input_details = tfl_interpreter.get_input_details()\n",
    "tfl_output_details = tfl_interpreter.get_output_details()\n",
    "\n",
    "# Convert PyTorch Input Tensor into Numpy Matrix and Reshape for TensorFlow\n",
    "tfl_interpreter.set_tensor(tfl_input_details[0]['index'], tf_input_data)\n",
    "tfl_interpreter.invoke()\n",
    "\n",
    "tfl_output_data = tfl_interpreter.get_tensor(tfl_output_details[0]['index'])\n",
    "\n",
    "probs = softmax(tfl_output_data[0])\n",
    "data = zip(range(len(probs)), probs)\n",
    "tfl_int8_results = [(categories[idx], prob) for (idx, prob) in sorted(data, key=lambda x: x[1], reverse=True)[:5]]\n",
    "for cat, prob in  tfl_int8_results:\n",
    "    print(cat, ':', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9ae5b",
   "metadata": {},
   "source": [
    "# Analysing Models\n",
    "\n",
    "Defined below is a function to print out the operator counts of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acc21b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "def get_operator_counts(model_content):\n",
    "    with io.StringIO() as buf, redirect_stdout(buf):\n",
    "        tf.lite.experimental.Analyzer.analyze(model_content=model_content)\n",
    "        model_structure = buf.getvalue()\n",
    "\n",
    "    operators = [op.strip().split(\" \")[1].split(\"(\")[0] for op in model_structure.split(\"\\n\") if \"Op#\" in op]\n",
    "    op_counts = {}\n",
    "    for operator in operators:\n",
    "        if operator in op_counts:\n",
    "            op_counts[operator] = op_counts[operator]+1\n",
    "        else:\n",
    "            op_counts[operator] = 1\n",
    "        \n",
    "    return (len(operators), op_counts)\n",
    "\n",
    "def print_operator_counts(model_content):\n",
    "    total_op_count, op_counts = get_operator_counts(model_content)\n",
    "    print(f\"{'Operator'.upper():<20} {'Count'.upper():>6}\")\n",
    "    print(\"-\"*20 + \" \" + \"-\"*6)\n",
    "    \n",
    "    for operator, count in op_counts.items():\n",
    "        print(f\"{operator.lower():<20} {count:>6}\")\n",
    "        \n",
    "    print(\"-\"*20 + \" \" + \"-\"*6)\n",
    "    print(f\"{'Total'.upper():<20} {total_op_count:>6}\")\n",
    "    print(\"-\"*20 + \" \" + \"-\"*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37ecb085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPERATOR              COUNT\n",
      "-------------------- ------\n",
      "quantize                  1\n",
      "pad                       5\n",
      "conv_2d                  35\n",
      "depthwise_conv_2d        17\n",
      "add                      10\n",
      "mean                      1\n",
      "fully_connected           1\n",
      "dequantize                1\n",
      "-------------------- ------\n",
      "TOTAL                    71\n",
      "-------------------- ------\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect the int8 model\n",
    "print_operator_counts(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b76da4",
   "metadata": {},
   "source": [
    "## Compare Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4834a427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1303/2000 = 65.14999999999999%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "\n",
    "# download labels\n",
    "resp = requests.get('https://raw.githubusercontent.com/tensorflow/models/master/research/slim/datasets/imagenet_metadata.txt')\n",
    "# make a dictionary of classes, e.g: {\"n00440382\": [\"broad\", \"jump\", \"long jump\"}\n",
    "labels_dict = {x[0]: [y.strip() for y in x[1].split(\",\")] for x in [x.split(\"\\t\") for x in resp.text.splitlines()]}\n",
    "\n",
    "\n",
    "# Construct a tf.data.Dataset\n",
    "ds, info = tfds.load('imagenette', split='validation', with_info=True, as_supervised=True, shuffle_files=True)\n",
    "label_codes = info.features['label']\n",
    "\n",
    "# make lowercase to standardise\n",
    "torch_labels = [x.lower() for x in categories]\n",
    "\n",
    "def is_label_in_both_datasets(labels):\n",
    "    return True in [imagenet_label.lower() in torch_labels for imagenet_label in labels]\n",
    "\n",
    "def is_correct_result(result, expected):\n",
    "    return result in expected\n",
    "\n",
    "\n",
    "# Build your input pipeline\n",
    "ds = ds.shuffle(2000).batch(1).prefetch(10).take(2000)\n",
    "\n",
    "iterations = 0\n",
    "correct = 0\n",
    "\n",
    "for image, label in ds:\n",
    "    # get actual words for the image classification\n",
    "    label_terms = labels_dict[label_codes.int2str(label.numpy()[0])]\n",
    "    \n",
    "    # only compare images where the label exists in both the dataset that the model was originally trained on, and in the tfds dataset\n",
    "    if is_label_in_both_datasets(label_terms):\n",
    "        iterations = iterations + 1\n",
    "        \n",
    "        img = tf.keras.utils.array_to_img(image[0])\n",
    "        pytorch_batch = preprocess(img).unsqueeze(0)\n",
    "        tf_batch = np.transpose( pytorch_batch.numpy(), [0, 2, 3, 1])\n",
    "\n",
    "        # use same interpreter as before\n",
    "        tfl_interpreter.set_tensor(tfl_input_details[0]['index'], tf_batch)\n",
    "        tfl_interpreter.invoke()\n",
    "\n",
    "        output = tfl_interpreter.get_tensor(tfl_output_details[0]['index'])\n",
    "\n",
    "        probs = softmax(output[0])\n",
    "        data = zip(range(len(probs)), probs)\n",
    "        (idx, prob) = sorted(data, key=lambda x: x[1], reverse=True)[0]\n",
    "        result = torch_labels[idx]\n",
    "        \n",
    "        if is_correct_result(result, label_terms):\n",
    "            correct = correct + 1\n",
    "\n",
    "accuracy = correct / iterations\n",
    "print(f\"Accuracy {correct}/{iterations} = {accuracy * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
